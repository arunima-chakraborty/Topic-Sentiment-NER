{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d03f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arunima.chakraborty/Documents/git/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import logging\n",
    "import contractions\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "import onnxruntime as ort\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "model_name = \"./distilbert-base-multilingual-cased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ef0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"./multilingual_model_onnx.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9c024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2ner_label = {\n",
    "            0:'O',\n",
    "            1:'PER',\n",
    "            2:'LOC',\n",
    "            3:'ORG',\n",
    "            4:'MISC'\n",
    "            }\n",
    "        \n",
    "id2bioes_label = {\n",
    "            0:'O',\n",
    "            1:'B',\n",
    "            2:'I',\n",
    "            3:'E',\n",
    "            4:'S'\n",
    "            \n",
    "        }\n",
    "id2sentiment_label = {\n",
    "            0:'negative',\n",
    "            1:'neutral',\n",
    "            2:'positive'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8906f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stop_words = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf95e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    text = \" \".join([contractions.fix(i) for i in words])\n",
    "    text = \" \".join([i for i in text.split() if i not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095f3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    onnx_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "    outputs = session.run(\n",
    "        [\"topic_logits\", \"ner_logits\", \"sentiment_logits\"],  \n",
    "        onnx_inputs  \n",
    "    )\n",
    "\n",
    "    topic_predictions = torch.argmax(torch.tensor(outputs[0]), dim=-1).squeeze().to(device).tolist()\n",
    "    ner_predictions = torch.argmax(torch.tensor(outputs[1]), dim=-1).squeeze().to(device).tolist()\n",
    "    sentiment_prediction = torch.argmax(torch.tensor(outputs[2])).to(device).item() \n",
    "    \n",
    "\n",
    "    return topic_predictions, sentiment_prediction,ner_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45664f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "    text = text_preprocessing(text)\n",
    "    preds = predictions(text)\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens = tokens[\"input_ids\"].squeeze().tolist()\n",
    "    words = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    topic_entities = []\n",
    "    topics, sentiment, ner = preds[0], preds[1], preds[2]\n",
    "    \n",
    "    for i in range(len(topics)):\n",
    "        if topics[i] != 0 and words[i] not in [\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\"] and ner[i] ==0:\n",
    "            topic_entities.append((i,words[i],topics[i]))\n",
    "    return text, topic_entities, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3a55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords(phrases):\n",
    "    res = []\n",
    "    for phrase in phrases:\n",
    "        tokens = phrase.split()\n",
    "        merged_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\") and merged_tokens:\n",
    "                \n",
    "                merged_tokens[-1] += token[2:]\n",
    "            else:\n",
    "                merged_tokens.append(token)\n",
    "\n",
    "        res.append(\" \".join(merged_tokens))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a8ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrases(tagged_entries):\n",
    "    phrases = []\n",
    "    current_phrase = []\n",
    "    last_index = -1\n",
    "\n",
    "    for idx, word, pred in tagged_entries:\n",
    "        if pred == 1:\n",
    "            if current_phrase:\n",
    "                phrases.append(' '.join(current_phrase))\n",
    "            current_phrase = [word]\n",
    "            last_index = idx\n",
    "        elif pred == 2:\n",
    "            if current_phrase and idx == last_index + 1:\n",
    "                current_phrase.append(word)\n",
    "                last_index = idx\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    if current_phrase:\n",
    "        phrases.append(' '.join(current_phrase))\n",
    "    phrases = merge_subwords(phrases)\n",
    "   \n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_text(text, max_words=50):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.strip().split()\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        if sum(len(s.split()) for s in current_chunk) + len(words) > max_words:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e75c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(lst):\n",
    "    seen = []\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.append(item)\n",
    "            result.append(item)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_extraction(text, keywords,sentiment):\n",
    "    if not isinstance(text, str) or not isinstance(keywords, list):\n",
    "        return []\n",
    "\n",
    "    original_words = text.split()\n",
    "    lowered_words = [w.lower() for w in original_words]\n",
    "    matched_phrases = []\n",
    "\n",
    "    for phrase in keywords:\n",
    "        phrase_words = phrase.lower().split()\n",
    "        i = 0\n",
    "        found = False\n",
    "\n",
    "        while i < len(lowered_words):\n",
    "            match = True\n",
    "            temp_indexes = []\n",
    "\n",
    "            for j, kw in enumerate(phrase_words):\n",
    "                if i + j >= len(lowered_words):\n",
    "                    match = False\n",
    "                    break\n",
    "                candidate = lowered_words[i + j]\n",
    "\n",
    "                \n",
    "                if kw == candidate or kw.strip('#') in candidate:\n",
    "                    temp_indexes.append(i + j)\n",
    "                else:\n",
    "                    match = False\n",
    "                    break\n",
    "\n",
    "            if match:\n",
    "                matched_text = ' '.join(original_words[idx] for idx in temp_indexes)\n",
    "                matched_phrases.append(matched_text)\n",
    "                found = True\n",
    "                break  \n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        if not found:\n",
    "            matched_phrases.append(phrase)\n",
    "\n",
    "    \n",
    "    final_phrases = []\n",
    "    for p in matched_phrases:\n",
    "        if not any(p in other and p != other for other in matched_phrases):\n",
    "            if p.lower() not in stop_words:\n",
    "                final_phrases.append(p)\n",
    "\n",
    "    if len(final_phrases) == 0:\n",
    "        return [], sentiment\n",
    "    final_phrases = [phrase for phrase in final_phrases if phrase.lower() not in stop_words]\n",
    "    keywords = remove_duplicates(final_phrases)\n",
    "    return keywords, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba71436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(text):\n",
    "    chunks = chunking_text(text)\n",
    "    final_topics = []\n",
    "    i = 0\n",
    "    final_sentiment = []\n",
    "    for chunk in chunks:\n",
    "        i+=1\n",
    "        _, entities, sentiment = predict_text(chunk)\n",
    "        entities = extract_phrases(entities)\n",
    "        keywords,sentiment = topic_extraction(text, entities,sentiment)\n",
    "        final_sentiment.append(sentiment)\n",
    "        final_topics.extend(keywords)\n",
    "\n",
    "    if len(final_sentiment) >0:\n",
    "        f = sum(final_sentiment)/len(final_sentiment)\n",
    "        if f > 0.5:\n",
    "            if f>1:\n",
    "                sentiment = 2\n",
    "            else:\n",
    "                sentiment = 1\n",
    "        else: \n",
    "            sentiment = 0\n",
    "    else: \n",
    "            sentiment = 1\n",
    "   \n",
    "\n",
    "    return final_topics, id2sentiment_label[sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d868af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Topics:\n",
      "['Apple Pay', 'alliance technology giant', 'cards accept payments', 'Apple Watch,', 'iPad,', 'innovative simple payment methods', 'promote digitalization colombians', 'Catalina Bretón,', 'general manager nu colombia', 'iPhone, Apple Watch,', 'contactless transaction.', 'secure authenticated face id', 'Touch ID,', \"device ' s access code\", 'well dynamic', 'one - time security code', 'convenience stores,', 'digitalization mechanism', 'contactless payment methods', 'better connect customers', 'reducing risks', 'verification information time card registration', 'open wallet app', 'bancolombia debit credit cards', 'adding card iphone apple watch', 'customers', 'apple pay device', 'Apple Pay', 'financial technology company', 'shopping habits', 'consumers moving away cash', 'digital contactless payment experiences', 'Apple Pay offers.', 'commitment secure digital payments', 'colombian cardholders safe seamless shopping experience', 'Federico Martínez,', 'country manager mastercard colombia']\n",
      "Sentiment:neutral\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Today, Apple Pay officially arrived in the country, and with the news, some financial institutions revealed their alliance with the technology giant to include cards and accept payments made with iPhone, Apple Watch, iPad, and Mac. Nu Colombia, Bancolombia, Mastercard, and Visa were the first to announce that they will be connected to the payment method. As a 100% digital company, we want to always offer the most innovative and simple payment methods to our customers and thus promote the digitalization of Colombians.\n",
    "Apple Pay is precisely a secure, convenient, simple, and highly innovative payment method. We are happy to announce that starting today, our customers can also make purchases using La Moradita through Apple Pay, said Catalina Bretón, General Manager of Nu Colombia. To make their payments, users must hold their iPhone or Apple Watch near a data terminal to make the contactless transaction. Every purchase with Apple Pay is secure because it is authenticated with Face ID, Touch ID, or the device's access code, as well as a dynamic, one-time security code. Apple Pay is accepted in supermarkets, convenience stores, pharmacies, restaurants, cafes, retail stores, and many more places. We believe in digitalization as a mechanism to accompany the life moments and needs of our customers. For more than seven years, we have participated in the evolution of contactless payment methods in the country, seeking to better connect with our customers and promoting the greater use of digital media, which, in a demanding context like the current one, represents not only ease but also contributes to reducing the risks associated with handling cash, says Cristina Arrastía, Vice President of Business at Bancolombia.\n",
    "To use Apple Pay, you only need to be enrolled in the bank's alerts and notifications service to receive the necessary verification information at the time of card registration. On the iPhone, simply open the Wallet app, tap, and follow the steps to add Bancolombia debit and credit cards. By adding the card to the iPhone or Apple Watch, the customer can immediately begin using Apple Pay on the device. Meanwhile, the card companies announced that Apple Pay is initially available to Bancolombia Mastercard and Visa cardholders and Nu Colombia, with other financial institutions soon to launch, the financial technology company emphasized. \"We are delighted to partner with our issuers to bring Apple Pay to Colombia. Our shopping habits are evolving. Consumers are moving away from cash more than ever and opting for faster digital and contactless payment experiences, and this is exactly what Apple Pay offers. With this launch, Mastercard continues to strengthen its commitment to more secure digital payments and offers Colombian cardholders a safe and seamless shopping experience,\" highlighted Federico Martínez, Country Manager for Mastercard in Colombia.\n",
    "\"\"\"\n",
    "keyword, sentiment = postprocessing(text)\n",
    "print(f\"Important Topics:\")\n",
    "print(keyword)\n",
    "print(f\"Sentiment:{sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
